sonicd {
  interface = "127.0.0.1"
  http-port = 9111
  tcp-port = 10001
  actor-timeout = 10000 //milliseconds
  endpoint-timeout = 30000 //milliseconds. Divided by 2 needs to be higher than actor-timeout

  auth-workers = 2
  auth-secret = "very_secret"
  token-duration = 60m
  api-keys = [{
    authorization = 1
    key = "1234"
    mode = "r"
    //from = all ips are allowed if not set
    //tokenExpires = uses global if not set
  }, {
    authorization = 10
    key = "1235"
    mode = "r"
    from = ["localhost"]
    tokenExpires {
      length = 1
      unit = HOURS //MINUTES/DAYS/HOURS.. or enum constant of java.util.concurrent.TimeUnit
    }
  }]

  ssl-config.ssl.debug.all = true
  ssl-config.ssl.loose.acceptAnyCertificate = true
  ssl-config.ssl.loose.allowWeakProtocols = true
  ssl-config.ssl.loose.acceptAnyCertificate = true
  ssl-config.ssl.default = true
  ssl-config.ssl.debug {
    ssl = true
    sslctx = true
    defaultctx = true
  }

  # Passed into SSLContext.getInstance()
  ssl-config.ssl.protocol = "TLSv1.2"

  # passed into sslContext.getDefaultParameters().setEnabledProtocols()
  ssl-config.ssl.enabledProtocols = [
    "TLSv1.2",
    "TLSv1.1",
  ]

  //server side configured sources
  source {
    secured_test {
      security = 3
      class = SyntheticSource
      seed = 100000
      progress-delay = 5
      size = 10
    }
    test {
      class = SyntheticSource
      seed = 100000
      progress-delay = 5
      size = 10
    }

    # server logs
    # note that this relies on logback config to
    # have a file appender that outputs logs
    # in JSON format at /var/log/opentok/sonicd.log
    # (which is the default)
    logs {
      class = LocalJsonStreamSource
      path = /var/log/sonicd/sonicd.log
      tail = true
    }
  }

  //JdbcSource
  jdbc {
    fetch-size = 1000
  }

  //PrestoSource
  presto {
    retry-in = 10s
    max-retries = 3
    timeout = 60s
    // for connection pool
    akka.http.parsing = ${akka.http.parsing}
    akka.http.client = {
      user-agent-header = sonicd/0.3.1
      connecting-timeout = 10s
      idle-timeout = 60 s
      request-header-size-hint = 512
      proxy {
        http = default
        https = default
      }
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }
      parsing {
      }
    }
    akka.http.host-connection-pool {
      max-connections = 24
      max-retries = 0
      max-open-requests = 128
      pipelining-limit = 1
      idle-timeout = 60 s
      client = ${sonicd.presto.akka.http.client}
    }
  }

  //ZuoraObjectQueryLanguageSource
  zuora {
    query-limit = 500
    endpoint = "/apps/services/a/75.0"
    query-timeout = 30s
    akka.http.parsing = ${akka.http.parsing}
    akka.http.client = {
      user-agent-header = sonicd/0.3.1
      connecting-timeout = 10s
      idle-timeout = 60 s
      request-header-size-hint = 512
      proxy {
        http = default
        https = default
      }
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }
      parsing {
      }
    }
    akka.http.host-connection-pool {
      max-connections = 1
      max-retries = 0
      max-open-requests = 1
      pipelining-limit = 1
      idle-timeout = 30 s
      client = ${sonicd.zuora.akka.http.client}
    }
  }

  //SparkSQLSource and SparkSource
  spark {
    home = "/usr/local/spark"
    //if running against standalone cluster, set to spark://host:port
    master = "local[2]"
    //optional, only needed if master = "yarn-client"
    driver.extraClassPath = "/etc/hadoop/conf:/etc/hive/conf:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-yarn/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*"
    driver.extraLibraryPath = "/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native"
    executor.extraLibraryPath = "/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native"
    executor.extraClassPath = "/etc/hadoop/conf:/etc/hive/conf:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-yarn/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*"
    jars = []
  }
}

akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"


  actor {

    debug {
      receive = on
      autoreceive = on
      lifecycle = on
    }

    //jdbc is blocking, so we give it a separate dispatcher
    //to avoid locking down the entire system
    jdbc-dispatcher {
      # Dispatcher is the name of the event-based dispatcher
      type = Dispatcher
      # What kind of ExecutionService to use
      executor = "thread-pool-executor"
      # Configuration for the thread pool
      thread-pool-executor {
        # minimum number of threads to cap factor-based core number to
        core-pool-size-min = 80
        # No of core threads ... ceil(available processors * factor)
        core-pool-size-factor = 80
        # maximum number of threads to cap factor-based number to
        core-pool-size-max = 400
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 1
    }
  }

  //  extensions = ["kamon.akka.Akka", "kamon.statsd.StatsD"]

  jvm-exit-on-fatal-error = off

  stream {
    # Default flow materializer settings
    materializer {

      # Initial size of buffers used in stream elements
      initial-input-buffer-size = 4
      # Maximum size of buffers used in stream elements
      max-input-buffer-size = 16

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by FlowMaterialiser when creating Actors.
      # When this value is left empty, the default-dispatcher will be used.
      dispatcher = ""

      # Cleanup leaked publishers and subscribers when they are not used within a given
      # deadline
      subscription-timeout {
        # when the subscription timeout is reached one of the following strategies on
        # the "stale" publisher:
        # cancel - cancel it (via `onError` or subscribing to the publisher and
        #          `cancel()`ing the subscription right away
        # warn   - log a warning statement about the stale element (then drop the
        #          reference to it)
        # noop   - do nothing (not recommended)
        mode = cancel

        # time after which a subscriber / publisher is considered stale and eligible
        # for cancelation (see `akka.stream.subscription-timeout.mode`)
        timeout = 5s
      }

      # Enable additional troubleshooting logging at DEBUG log level
      debug-logging = on

      # Maximum number of elements emitted in batch if downstream signals large demand
      output-burst-limit = 1000

      # Enable automatic fusing of all graphs that are run. For short-lived streams
      # this may cause an initial runtime overhead, but most of the time fusing is
      # desirable since it reduces the number of Actors that are created.
      auto-fusing = on

      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
      # buffer upon stream materialization if the requested buffer size is less than this
      # configuration parameter. The default is very high because failing early is better
      # than failing under load.
      #
      # Buffers sized larger than this will dynamically grow/shrink and consume more memory
      # per element than the fixed size buffers.
      max-fixed-buffer-size = 1000000000

      # Maximum number of sync messages that actor can process for stream to substream communication.
      # Parameter allows to interrupt synchronous processing to get upsteam/downstream messages.
      # Allows to accelerate message processing that happening withing same actor but keep system responsive.
      sync-processing-limit = 1000

      debug {
        # Enables the fuzzing mode which increases the chance of race conditions
        # by aggressively reordering events and making certain operations more
        # concurrent than usual.
        # This setting is for testing purposes, NEVER enable this in a production
        # environment!
        # To get the best results, try combining this setting with a throughput
        # of 1 on the corresponding dispatchers.
        fuzzing-mode = off
      }
    }

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by FlowMaterialiser when creating Actors for IO operations,
    # such as FileSource, FileSink and others.
    blocking-io-dispatcher = "akka.stream.default-blocking-io-dispatcher"

    default-blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      throughput = 1

      thread-pool-executor {
        core-pool-size-min = 2
        core-pool-size-factor = 2.0
        core-pool-size-max = 16
      }
    }
  }

  # configure overrides to ssl-configuration here (to be used by akka-streams, and akka-http – i.e. when serving https connections)
  ssl-config {
    # due to still supporting JDK6 in this release
    protocol = "TLSv1.2"
  }

  http {

    server {
      # The default value of the `Server` header to produce if no
      # explicit `Server`-header was included in a response.
      # If this value is the empty string and no header was included in
      # the request, no `Server` header will be rendered at all.
      server-header = akka-http/${akka.version}

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = infinite

      # The time period within which the TCP binding process must be completed.
      # Set to `infinite` to disable.
      bind-timeout = 1s

      # The maximum number of concurrently accepted connections when using the
      # `Http().bindAndHandle` methods.
      #
      # This setting doesn't apply to the `Http().bind` method which will still
      # deliver an unlimited backpressured stream of incoming connections.
      max-connections = 1024

      # The maximum number of requests that are accepted (and dispatched to
      # the application) on one single connection before the first request
      # has to be completed.
      # Incoming requests that would cause the pipelining limit to be exceeded
      # are not read from the connections socket so as to build up "back-pressure"
      # to the client via TCP flow control.
      # A setting of 1 disables HTTP pipelining, since only one request per
      # connection can be "open" (i.e. being processed by the application) at any
      # time. Set to higher values to enable HTTP pipelining.
      # This value must be > 0 and <= 1024.
      pipelining-limit = 24

      # Enables/disables the addition of a `Remote-Address` header
      # holding the clients (remote) IP address.
      remote-address-header = on

      # Enables/disables the addition of a `Raw-Request-URI` header holding the
      # original raw request URI as the client has sent it.
      raw-request-uri-header = off

      # Enables/disables automatic handling of HEAD requests.
      # If this setting is enabled the server dispatches HEAD requests as GET
      # requests to the application and automatically strips off all message
      # bodies from outgoing responses.
      # Note that, even when this setting is off the server will never send
      # out message bodies on responses to HEAD requests.
      transparent-head-requests = on

      # Enables/disables the returning of more detailed error messages to
      # the client in the error response.
      # Should be disabled for browser-facing APIs due to the risk of XSS attacks
      # and (probably) enabled for internal or non-browser APIs.
      # Note that akka-http will always produce log messages containing the full
      # error details.
      verbose-error-messages = off

      # The initial size of the buffer to render the response headers in.
      # Can be used for fine-tuning response rendering performance but probably
      # doesn't have to be fiddled with in most applications.
      response-header-size-hint = 512

      # The requested maximum length of the queue of incoming connections.
      # If the server is busy and the backlog is full the OS will start dropping
      # SYN-packets and connection attempts may fail. Note, that the backlog
      # size is usually only a maximum size hint for the OS and the OS can
      # restrict the number further based on global limits.
      backlog = 100

      # If this setting is empty the server only accepts requests that carry a
      # non-empty `Host` header. Otherwise it responds with `400 Bad Request`.
      # Set to a non-empty value to be used in lieu of a missing or empty `Host`
      # header to make the server accept such requests.
      # Note that the server will never accept HTTP/1.1 request without a `Host`
      # header, i.e. this setting only affects HTTP/1.1 requests with an empty
      # `Host` header as well as HTTP/1.0 requests.
      # Examples: `www.spray.io` or `example.com:8080`
      default-host-header = ""

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }

      # Modify to tweak parsing settings on the server-side only.
      parsing {
        # no overrides by default, see `akka.http.parsing` for default values
      }
    }

    client {
      # The default value of the `User-Agent` header to produce if no
      # explicit `User-Agent`-header was included in a request.
      # If this value is the empty string and no header was included in
      # the request, no `User-Agent` header will be rendered at all.
      user-agent-header = akka-http/${akka.version}

      # The time period within which the TCP connecting process must be completed.
      connecting-timeout = 10s

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle timeouts.
      idle-timeout = infinite

      # The initial size of the buffer to render the request headers in.
      # Can be used for fine-tuning request rendering performance but probably
      # doesn't have to be fiddled with in most applications.
      request-header-size-hint = 512

      # The proxy configurations to be used for requests with the specified
      # scheme.
      proxy {
        # Proxy settings for unencrypted HTTP requests
        # Set to 'none' to always connect directly, 'default' to use the system
        # settings as described in http://docs.oracle.com/javase/6/docs/technotes/guides/net/proxies.html
        # or specify the proxy host, port and non proxy hosts as demonstrated
        # in the following example:
        # http {
        #   host = myproxy.com
        #   port = 8080
        #   non-proxy-hosts = ["*.direct-access.net"]
        # }
        http = default

        # Proxy settings for HTTPS requests (currently unsupported)
        https = default
      }

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }

      # Modify to tweak parsing settings on the client-side only.
      parsing {
        # no overrides by default, see `akka.http.parsing` for default values
      }
    }

    host-connection-pool {
      # The maximum number of parallel connections that a connection pool to a
      # single host endpoint is allowed to establish. Must be greater than zero.
      max-connections = 4

      # The maximum number of times failed requests are attempted again,
      # (if the request can be safely retried) before giving up and returning an error.
      # Set to zero to completely disable request retries.
      max-retries = 5

      # The maximum number of open requests accepted into the pool across all
      # materializations of any of its client flows.
      # Protects against (accidentally) overloading a single pool with too many client flow materializations.
      # Note that with N concurrent materializations the max number of open request in the pool
      # will never exceed N * max-connections * pipelining-limit.
      # Must be a power of 2 and > 0!
      max-open-requests = 32

      # The maximum number of requests that are dispatched to the target host in
      # batch-mode across a single connection (HTTP pipelining).
      # A setting of 1 disables HTTP pipelining, since only one request per
      # connection can be "in flight" at any time.
      # Set to higher values to enable HTTP pipelining.
      # This value must be > 0.
      # (Note that, independently of this setting, pipelining will never be done
      # on a connection that still has a non-idempotent request in flight.
      # See http://tools.ietf.org/html/rfc7230#section-6.3.2 for more info.)
      pipelining-limit = 1

      # The time after which an idle connection pool (without pending requests)
      # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
      idle-timeout = infinite

      # Modify to tweak client settings for host connection pools only.
      #
      # IMPORTANT:
      # Please note that this section mirrors `akka.http.client` however is used only for pool-based APIs,
      # such as `Http().superPool` or `Http().singleRequest`.
      client = {
        # The default value of the `User-Agent` header to produce if no
        # explicit `User-Agent`-header was included in a request.
        # If this value is the empty string and no header was included in
        # the request, no `User-Agent` header will be rendered at all.
        user-agent-header = akka-http/${akka.version}

        # The time period within which the TCP connecting process must be completed.
        connecting-timeout = 10s

        # The time after which an idle connection will be automatically closed.
        # Set to `infinite` to completely disable idle timeouts.
        idle-timeout = infinite

        # The initial size of the buffer to render the request headers in.
        # Can be used for fine-tuning request rendering performance but probably
        # doesn't have to be fiddled with in most applications.
        request-header-size-hint = 512

        # The proxy configurations to be used for requests with the specified
        # scheme.
        proxy {
          # Proxy settings for unencrypted HTTP requests
          # Set to 'none' to always connect directly, 'default' to use the system
          # settings as described in http://docs.oracle.com/javase/6/docs/technotes/guides/net/proxies.html
          # or specify the proxy host, port and non proxy hosts as demonstrated
          # in the following example:
          # http {
          #   host = myproxy.com
          #   port = 8080
          #   non-proxy-hosts = ["*.direct-access.net"]
          # }
          http = default

          # Proxy settings for HTTPS requests (currently unsupported)
          https = default
        }

        # Socket options to set for the listening socket. If a setting is left
        # undefined, it will use whatever the default on the system is.
        socket-options {
          so-receive-buffer-size = undefined
          so-send-buffer-size = undefined
          so-reuse-address = undefined
          so-traffic-class = undefined
          tcp-keep-alive = undefined
          tcp-oob-inline = undefined
          tcp-no-delay = undefined
        }


        # IMPORTANT: Please note that this section is replicated in `client` and `server`.
        parsing {
          # no overrides by default, see `akka.http.parsing` for default values
        }
      }
    }

    # Modify to tweak default parsing settings.
    #
    # IMPORTANT:
    # Please note that this sections settings can be overriden by the corresponding settings in:
    # `akka.http.server.parsing`, `akka.http.client.parsing` or `akka.http.http-connection-pool.client.parsing`.
    parsing {
      # The limits for the various parts of the HTTP message parser.
      max-uri-length = 2k
      max-method-length = 16
      max-response-reason-length = 64
      max-header-name-length = 64
      max-header-value-length = 8k
      max-header-count = 64
      max-chunk-ext-length = 256
      max-chunk-size = 1m

      # Maximum content length which should not be exceeded by incoming HttpRequests.
      # For file uploads which use the entityBytes Source of an incoming HttpRequest it is safe to
      # set this to a very high value (or to `infinite` if feeling very adventurous) as the streaming
      # upload will be back-pressured properly by Akka Streams.
      # Please note however that this setting is a global property, and is applied to all incoming requests,
      # not only file uploads consumed in a streaming fashion, so pick this limit wisely.
      max-content-length = 8m

      # Sets the strictness mode for parsing request target URIs.
      # The following values are defined:
      #
      # `strict`: RFC3986-compliant URIs are required,
      #     a 400 response is triggered on violations
      #
      # `relaxed`: all visible 7-Bit ASCII chars are allowed
      #
      uri-parsing-mode = strict

      # Sets the parsing mode for parsing cookies.
      # The following value are defined:
      #
      # `rfc6265`: Only RFC6265-compliant cookies are parsed. Surrounding double-quotes are accepted and
      #   automatically removed. Non-compliant cookies are silently discarded.
      # `raw`: Raw parsing allows any non-control character but ';' to appear in a cookie value. There's no further
      #   post-processing applied, so that the resulting value string may contain any number of whitespace, unicode,
      #   double quotes, or '=' characters at any position.
      #   The rules for parsing the cookie name are the same ones from RFC 6265.
      #
      cookie-parsing-mode = rfc6265

      # Enables/disables the logging of warning messages in case an incoming
      # message (request or response) contains an HTTP header which cannot be
      # parsed into its high-level model class due to incompatible syntax.
      # Note that, independently of this settings, akka-http will accept messages
      # with such headers as long as the message as a whole would still be legal
      # under the HTTP specification even without this header.
      # If a header cannot be parsed into a high-level model instance it will be
      # provided as a `RawHeader`.
      # If logging is enabled it is performed with the configured
      # `error-logging-verbosity`.
      illegal-header-warnings = on

      # Configures the verbosity with which message (request or response) parsing
      # errors are written to the application log.
      #
      # Supported settings:
      # `off`   : no log messages are produced
      # `simple`: a condensed single-line message is logged
      # `full`  : the full error details (potentially spanning several lines) are logged
      error-logging-verbosity = full

      # limits for the number of different values per header type that the
      # header cache will hold
      header-cache {
        default = 12
        Content-MD5 = 0
        Date = 0
        If-Match = 0
        If-Modified-Since = 0
        If-None-Match = 0
        If-Range = 0
        If-Unmodified-Since = 0
        User-Agent = 32
      }
    }
  }
}

kamon {

  metric {

    # Time interval for collecting all metrics and send the snapshots to all subscribed actors.
    tick-interval = 10 seconds

    # Disables a big error message that will be typically logged if your application wasn't started
    # with the -javaagent:/path-to-aspectj-weaver.jar option. If you are only using KamonStandalone
    # it might be ok for you to turn this error off.
    disable-aspectj-weaver-missing-error = false

    # Specify if entities that do not match any include/exclude filter should be tracked.
    track-unmatched-entities = yes

    filters {
      akka-actor {
        includes = ["sonic/user/*", "sonic/user/*/*", "sonic/user/*/*/*"]
        excludes = ["*/system/*", "*kamon*"]
      }

      akka-router {
        includes = ["sonic/user/*"]
        excludes = []
      }

      akka-dispatcher {
        includes = ["sonic/user/*"]
        excludes = []
      }

      trace {
        includes = ["*"]
        excludes = []
      }
    }
  }

  # Controls whether the AspectJ Weaver missing warning should be displayed if any Kamon module requiring AspectJ is
  # found in the classpath but the application is started without the AspectJ Weaver.
  show-aspectj-missing-warning = yes

  statsd {

    # Hostname and port in which your StatsD is running. Remember that StatsD packets are sent using UDP and
    # setting unreachable hosts and/or not open ports wont be warned by the Kamon, your data wont go anywhere.
    hostname = "192.168.99.100"
    port = 8125

    # Interval between metrics data flushes to StatsD. It's value must be equal or greater than the
    # kamon.metric.tick-interval setting.
    flush-interval = 10 seconds

    # Max packet size for UDP metrics data sent to StatsD.
    max-packet-size = 1024 bytes

    # Subscription patterns used to select which metrics will be pushed to StatsD. Note that first, metrics
    # collection for your desired entities must be activated under the kamon.metrics.filters settings.
    subscriptions {
      histogram = ["**"]
      min-max-counter = ["**"]
      gauge = ["**"]
      counter = ["**"]
      trace = ["**"]
      trace-segment = ["**"]
      akka-actor = ["**"]
      akka-dispatcher = ["**"]
      akka-router = ["**"]
      system-metric = ["**"]
      http-server = ["**"]
    }

    # FQCN of the implementation of `kamon.statsd.MetricKeyGenerator` to be instantiated and used for assigning
    # metric names. The implementation must have a single parameter constructor accepting a `com.typesafe.config.Config`.
    metric-key-generator = kamon.statsd.SimpleMetricKeyGenerator

    simple-metric-key-generator {

      # Application prefix for all metrics pushed to StatsD. The default namespacing scheme for metrics follows
      # this pattern:
      #    application.host.entity.entity-name.metric-name
      application = "sonic"

      # Includes the name of the hostname in the generated metric. When set to false, the scheme for the metrics
      # will look as follows:
      #    application.entity.entity-name.metric-name
      include-hostname = true

      # Allow users to override the name of the hostname reported by kamon. When changed, the scheme for the metrics
      # will have the following pattern:
      #   application.hostname-override-value.entity.entity-name.metric-name
      hostname-override = none

      # When the sections that make up the metric names have special characters like dots (very common in dispatcher
      # names) or forward slashes (all actor metrics) we need to sanitize those values before sending them to StatsD
      # with one of the following strategies:
      #   - normalize: changes ': ' to '-' and ' ', '/' and '.' to '_'.
      #   - percent-encode: percent encode the section on the metric name. Please note that StatsD doesn't support
      #     percent encoded metric names, this option is only useful if using our docker image which has a patched
      #     version of StatsD or if you are running your own, customized version of StatsD that supports this.
      metric-name-normalization-strategy = normalize
    }
  }

  # modules can be disabled at startup using yes/no arguments.
  modules {
    kamon-log-reporter.auto-start = yes
    kamon-system-metrics.auto-start = yes
    kamon-statsd.auto-start = yes
    kamon-akka.auto-start = yes
  }

}
